{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243109b0-bcb5-4998-87b8-3bc0b027aaf2",
   "metadata": {},
   "source": [
    "**Few-Shot Prompting**\n",
    "\n",
    "\t• What it is: You give the model a few examples of context → question → answer, then add your new context + question.\n",
    "\t• When to use:\n",
    "\t• When you want the model to mimic a specific style (tone, structure, format).\n",
    "\t• If you want consistent answers (e.g., always ending with “Sources: …”).\n",
    "\t• Good for teaching the model special formatting rules or domain-specific style.\n",
    "\t• Trade-off: More control, but you must prepare good examples, and the prompt can get longer.\n",
    "\n",
    "Example:\n",
    "\n",
    "Show 2–3 sample Q&A pairs → Then ask a new question with new context.\n",
    "The model copies the answering style from your examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8159e7e8-d79b-426a-aa7e-98eeff5fe876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U llama-index llama-index-llms-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f55a06a-faf5-4ff4-b559-1807e10d721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FEW-SHOT ---\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\"\n",
    "\n",
    "def run_llm_fewshot(context: str, query: str, examples: List[Dict[str, str]]) -> str:\n",
    "    llm = Groq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "\n",
    "    examples_str = \"\\n\".join(\n",
    "        f\"Example {i+1}:\\nContext:\\n{ex.get('context','')}\\n\"\n",
    "        f\"Question: {ex.get('question','')}\\n\"\n",
    "        f\"Answer: {ex.get('answer','')}\\n\"\n",
    "        for i, ex in enumerate(examples)\n",
    "    )\n",
    "\n",
    "    template_str = (\n",
    "        \"You are an expert AI assistant.\\n\"\n",
    "        \"Use ONLY the provided context to answer the user's question. \"\n",
    "        \"If the context is insufficient or does not mention the answer, reply exactly: \"\n",
    "        \"'Not enough information.'\\n\\n\"\n",
    "        \"Follow the style and reasoning illustrated by the examples.\\n\\n\"\n",
    "        \"Examples:\\n{examples_str}\\n\"\n",
    "        \"--- End of Examples ---\\n\\n\"\n",
    "        \"Context:\\n{context_str}\\n\\n\"\n",
    "        \"User Question: {query_str}\\n\\n\"\n",
    "        \"Answering Rules:\\n\"\n",
    "        \"1) Be concise and precise (3–6 sentences, unless the question requires more).\\n\"\n",
    "        \"2) Use bullet points for lists.\\n\"\n",
    "        \"3) At the end, include a 'Sources:' section with short snippets or filenames from the context you used.\\n\\n\"\n",
    "        \"Final Answer:\"\n",
    "    )\n",
    "    prompt = PromptTemplate(template_str).format(\n",
    "        examples_str=examples_str, context_str=context, query_str=query\n",
    "    )\n",
    "    return llm.complete(prompt=prompt).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0748ae19-61a1-40da-b8d3-d38e6d3104c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the attention mechanism, softmax is applied to the similarity scores between queries and keys. This process produces attention weights, which represent the relative importance of each key with respect to the query. The softmax function normalizes the similarity scores, ensuring they add up to 1. This normalization allows the model to focus on the most relevant keys. Key benefits of using softmax include:\n",
      "* Normalizing the attention weights\n",
      "* Ensuring the weights add up to 1\n",
      "Sources: attention_mechanism.pdf\n"
     ]
    }
   ],
   "source": [
    "# Few-Shot: Add examples so the model mimics your style\n",
    "shots = [\n",
    "    {\n",
    "        \"context\": \"Positional encodings inject order information into sequences.\",\n",
    "        \"question\": \"Why are positional encodings needed?\",\n",
    "        \"answer\": (\n",
    "            \"They give the model a sense of word order.\\n\"\n",
    "            \"- Without them, the model treats tokens as a bag of words.\\n\"\n",
    "            \"- Encodings ensure the sequence structure is preserved.\\n\"\n",
    "            \"Sources: lecture_notes.txt\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Multi-head attention projects queries, keys, and values into multiple subspaces.\",\n",
    "        \"question\": \"What is the benefit of multi-head attention?\",\n",
    "        \"answer\": (\n",
    "            \"It lets the model learn from different representation subspaces.\\n\"\n",
    "            \"- Captures diverse relationships.\\n\"\n",
    "            \"- Improves contextual understanding.\\n\"\n",
    "            \"Sources: attention_paper.pdf\"\n",
    "        )\n",
    "    },\n",
    "]\n",
    "\n",
    "context_text = (\n",
    "    \"Context from attention_mechanism.pdf\"\n",
    "    \"In the attention mechanism, softmax is used on the similarity scores \"\n",
    "    \"between queries and keys to produce attention weights.\"\n",
    ")\n",
    "\n",
    "query_text = \"What does softmax do in attention?\"\n",
    "\n",
    "ansk = run_llm_fewshot(context=context_text, query=query_text, examples=shots)\n",
    "\n",
    "print(ansk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6b7ad-6a39-48c5-8842-acce7941d915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98930af5-5518-42d6-bc87-2e8a56965dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e2f2ffe-1ba2-4b1c-a2cb-1a6ee07211b3",
   "metadata": {},
   "source": [
    "**👉 Rule of thumb:**\n",
    "\n",
    "\t• Use Zero-Shot for quick, flexible answers.\n",
    "\t• Use Few-Shot when consistency, formatting, or special style matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eced134-ce75-4e97-ab18-05fa6f132bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
